ToDo

	---KNOWN ISSUES------------------------------------------------
		
		program control
			possible solution - change queue items to individual tasks rather than one host
			PAUSE functionality (in-program)
			STOP file (for resuming scans)
			Ctrl+C 'graceful stop' effectiveness
			need a better timeout mechanism for spider and diagram creation
		html result to thumbnail matchup issues when scanning external websites w/ multiple handles (IP, hn, port)		
		bing --dns doubles up on targets (inadvertantly pulls the same hosts for each port#)
		


	---FUNCTIONALITY-----------------------------------------------

		dynamic thread count based on RAM availability
		cleanup
			remove raw response object from target - give the option to keep it?
			remove large/unneeded items from target before insertion into db
			functions.py
				SiThread
				parsedata - initial cleanup
				crawl
				parse_qualys_scan_report_xml - initial cleanup
				parse_nessus_xml - initial cleanup
				parse_nmap_xml - initial cleanup
		filter valid element names from wordlists
		finish DNS axfr/query function
		add option to use google dorks to pull common docs (crawl or not)
		postgres compatibility
		accept --useragent csv input
		smarter redirects + history
			if res.status_code == 302 and not domainname in res.get_redirect_location:
				target['redir_loc'] = res.get_redirect_location
			else: follow redirect!
			target['history'] to record all redirect urls
		better crawling
			implement spider_url_hit_limit
			need a better timeout mechanism for spider and diagram creation
			reduce hard-limit for # of links in diagram
			
		

	---FUTURE-----------------------------------------------------
		implement curses interface
		stats page in HTML report
        	div after HTML 'data' div
        	stats ideas: server version, SSL, ports, service (PHP, ASP.NET)
		scan comparison functionality
			compare two scans within one db
			compare a db scan with new scan
			'diff' HTML report
